{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# system libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Add the src directory to the path in order to import config\n",
    "current_directory = Path.cwd()\n",
    "src_path = current_directory.parent / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# project files\n",
    "import config\n",
    "import load_option_data_01 as l1\n",
    "import filter_option_data_01 as f1\n",
    "import wrds\n",
    "import bsm_pricer as bsm\n",
    "\n",
    "# environment variables\n",
    "WRDS_USERNAME = Path(config.WRDS_USERNAME)\n",
    "DATA_DIR = Path(config.DATA_DIR)\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_store_curve(group):\n",
    "    \"\"\"\n",
    "    Fit a quadratic curve to the given group of data points and store the fitted values.\n",
    "\n",
    "    Args:\n",
    "        group (pandas.DataFrame): The group of data points to fit the curve to.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The group of data points with the fitted values stored in the 'fitted_iv' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fit the quadratic curve\n",
    "        coefficients = np.polyfit(group['mnyns'], group['log_iv'], 2)\n",
    "        # Calculate fitted values\n",
    "        group['fitted_iv'] = np.polyval(coefficients, group['mnyns'])\n",
    "    except np.RankWarning:\n",
    "        print(\"Polyfit may be poorly conditioned\")\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_distance(series1, series2, method='percent'):\n",
    "    \"\"\"\n",
    "    Calculate the relative distance between the implied volatility and the fitted implied volatility.\n",
    "    \n",
    "    Parameters:\n",
    "        method (str): The method to calculate the relative distance. Options are 'percent', 'manhattan', or 'euclidean'.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The relative distance calculated based on the specified method.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the method is not one of 'percent', 'manhattan', or 'euclidean'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'percent':\n",
    "        result = (series1 - series2) / series2 * 100\n",
    "    elif method == 'manhattan':\n",
    "        result = abs(series1 - series2)\n",
    "    elif method == 'euclidean':\n",
    "        result = np.sqrt((series1 - series2)**2)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'percent', 'manhattan', or 'euclidean'\")\n",
    "    \n",
    "    result = np.where(np.isinf(result), np.nan, result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def mark_outliers(row, std_devs, outlier_threshold):\n",
    "    \"\"\"\n",
    "    Determines if a data point is an outlier based on its moneyness_bin and relative distance from the fitted curve.\n",
    "    \n",
    "    Args:\n",
    "        row (pandas.Series): A row of data containing the moneyness_bin and rel_distance columns.\n",
    "        std_devs (pandas.DataFrame): A DataFrame containing the standard deviations for each moneyness_bin.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the data point is an outlier, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attempt to retrieve the standard deviation for the row's moneyness_bin\n",
    "    std_dev_row = std_devs.loc[std_devs['mnyns_bin'] == row['mnyns_bin'], 'std_dev']\n",
    "    \n",
    "    # Check if std_dev_row is empty (i.e., no matching moneyness_bin was found)\n",
    "    if not std_dev_row.empty:\n",
    "        std_dev = std_dev_row.values[0]\n",
    "        # Calculate how many std_devs away from the fitted curve the IV is\n",
    "        if abs(row['rel_distance']) > outlier_threshold * std_dev:  # Adjust this threshold as needed\n",
    "            return True\n",
    "    else:\n",
    "        # Handle the case where no matching moneyness_bin was found\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_put_call_pairs(call_options, put_options):\n",
    "    \"\"\"\n",
    "    Builds pairs of call and put options based on the same date, expiration date, and moneyness.\n",
    "\n",
    "    Args:\n",
    "        call_options (DataFrame): DataFrame containing call options data.\n",
    "        put_options (DataFrame): DataFrame containing put options data.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (matching_calls: pd.DataFrame, matching_puts: pd.DataFrame)\n",
    "    \"\"\"\n",
    "    call_options.set_index(['date', 'exdate', 'mnyns'], inplace=True)\n",
    "    put_options.set_index(['date', 'exdate', 'mnyns'], inplace=True)\n",
    "    \n",
    "    # get common indices\n",
    "    common_index = call_options.index.intersection(put_options.index)\n",
    "\n",
    "    # Extract the matching entries\n",
    "    matching_calls = call_options.loc[common_index]\n",
    "    matching_puts = put_options.loc[common_index]\n",
    "    \n",
    "    result = (matching_calls, matching_puts)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def test_price_strike_match(matching_calls_puts):\n",
    "    \"\"\"\n",
    "    Check if the strike prices and security prices of matching calls and puts are equal.\n",
    "\n",
    "    Parameters:\n",
    "    matching_calls_puts (DataFrame): DataFrame containing matching calls and puts data.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the strike prices and security prices of matching calls and puts are equal, False otherwise.\n",
    "    \"\"\"\n",
    "    return (np.allclose(matching_calls_puts['strike_price_C'], matching_calls_puts['strike_price_P'])) and (np.allclose(matching_calls_puts['sec_price_C'], matching_calls_puts['sec_price_P']))# and (np.allclose(matching_calls_puts['tb_m3_C'], matching_calls_puts['tb_m3_P']))\n",
    "\n",
    "\n",
    "def calc_implied_interest_rate(matched_options):\n",
    "    \"\"\"\n",
    "    Calculates the implied interest rate based on the given matched options data.\n",
    "\n",
    "    Parameters:\n",
    "    matched_options (DataFrame): DataFrame containing the matched options data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with an additional column 'pc_parity_int_rate' representing the implied interest rate.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If there is a mismatch between the price and strike price of the options.\n",
    "    \"\"\"\n",
    "    \n",
    "    # underlying price\n",
    "    if test_price_strike_match(matched_options):\n",
    "        print(\">> Underlying prices, strike prices of put and call options match exactly.\")\n",
    "        S = matched_options['sec_price_C']\n",
    "        K = matched_options['strike_price_C']  \n",
    "        \n",
    "        # 1/T = 1/time to expiration in years\n",
    "        T_inv = np.power((matched_options.reset_index()['exdate']-matched_options.reset_index()['date'])/datetime.timedelta(days=365), -1)\n",
    "        T_inv.index=matched_options.index\n",
    "        T_inv\n",
    "        \n",
    "        C_mid = matched_options['mid_price_C']\n",
    "        P_mid = matched_options['mid_price_P']\n",
    "        # implied interest rate\n",
    "        matched_options['pc_parity_int_rate'] = np.log((S-C_mid+P_mid)/K) * T_inv\n",
    "        return matched_options\n",
    "    else:\n",
    "        raise ValueError(\"Price and strike price mismatch\")\n",
    "\n",
    "\n",
    "def pcp_filter_outliers(matched_options, int_rate_rel_distance_func, outlier_threshold):\n",
    "    \"\"\"\n",
    "    Filters out outliers based on the relative distance of interest rates and the outlier threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - matched_options (DataFrame): DataFrame containing the matched options data.\n",
    "    - int_rate_rel_distance_func (str): Method to calculate the relative distance of interest rates.\n",
    "    - outlier_threshold (float): Threshold for flagging outliers.\n",
    "\n",
    "    Returns:\n",
    "    - l3_filtered_options (DataFrame): DataFrame with outliers filtered out.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    matched_options['rel_distance_int_rate'] = calc_relative_distance(matched_options['pc_parity_int_rate'], matched_options['daily_median_rate'], method=int_rate_rel_distance_func)\n",
    "    # fill 3905 nans...\n",
    "    matched_options['rel_distance_int_rate'] = matched_options['rel_distance_int_rate'].fillna(0.0)\n",
    "\n",
    "    # calculate the standard deviation of the relative distances\n",
    "    stdev_int_rate_rel_distance = matched_options['rel_distance_int_rate'].std()\n",
    "\n",
    "    # flag outliers based on the threshold\n",
    "    matched_options['is_outlier_int_rate'] = matched_options['rel_distance_int_rate'].abs() > outlier_threshold * stdev_int_rate_rel_distance\n",
    "\n",
    "    # filter out the outliers\n",
    "    l3_filtered_options = matched_options[~matched_options['is_outlier_int_rate']]\n",
    "\n",
    "    # make the dataframe long-form to compare to the level 2 data\n",
    "    _calls = l3_filtered_options.filter(like='_C').rename(columns=lambda x: x.replace('_C', ''))\n",
    "    _puts = l3_filtered_options.filter(like='_P').rename(columns=lambda x: x.replace('_P', ''))\n",
    "    l3_filtered_options = pd.concat((_calls, _puts), axis=0)\n",
    "\n",
    "    # update the final results for this combination of relative distance method and outlier threshold\n",
    "    return l3_filtered_options\n",
    "\n",
    "\n",
    "def iv_filter_outliers(l2_data, iv_distance_method, iv_outlier_threshold):\n",
    "    \"\"\"\n",
    "    Filter out outliers based on the relative distance of log_iv and fitted_iv.\n",
    "\n",
    "    Parameters:\n",
    "    l2_data (DataFrame): Input data containing log_iv, fitted_iv, mnyns columns.\n",
    "    iv_distance_method (str): Method to calculate relative distance of log_iv and fitted_iv.\n",
    "    iv_outlier_threshold (float): Threshold value to flag outliers.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Filtered data without outliers.\n",
    "\n",
    "    \"\"\"\n",
    "    l2_data['rel_distance_iv'] = calc_relative_distance(l2_data['log_iv'], l2_data['fitted_iv'], method=iv_distance_method)\n",
    "\n",
    "    # Define moneyness bins\n",
    "    bins = np.arange(0.8, 1.21, 0.05)\n",
    "    l2_data['mnyns_bin'] = pd.cut(l2_data['mnyns'], bins=bins)\n",
    "\n",
    "    # Compute standard deviation of relative distances within each moneyness bin\n",
    "    std_devs = l2_data.groupby('mnyns_bin')['rel_distance_iv'].std().reset_index(name='std_dev')\n",
    "    \n",
    "    l2_data['stdev_iv_mnyns_bin'] = l2_data['mnyns_bin'].map(std_devs.set_index('mnyns_bin')['std_dev'])\n",
    "    l2_data['stdev_iv_mnyns_bin'].apply(lambda x: x*iv_outlier_threshold).astype(float)\n",
    "    # flag outliers based on the threshold\n",
    "    l2_data['is_outlier_iv'] = l2_data['rel_distance_iv'].abs() > l2_data['stdev_iv_mnyns_bin'].apply(lambda x: x*iv_outlier_threshold).astype(float)\n",
    "\n",
    "    # filter out the outliers\n",
    "    l3_data_iv_only = l2_data[~l2_data['is_outlier_iv']]\n",
    "    \n",
    "    # update the final results for this combination of relative distance method and outlier threshold\n",
    "    return l3_data_iv_only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table B1 Results for Level 3 Filter (per paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = pd.DataFrame(index=pd.MultiIndex.from_product([['Level 3 filters'], ['IV filter', 'Put-call parity filter', 'All']]),\n",
    "                             columns=pd.MultiIndex.from_product([['Berkeley', 'OptionMetrics'], ['Deleted', 'Remaining']]))\n",
    "check_results.loc[['Level 3 filters'], ['Berkeley', 'OptionMetrics']] = [[10865, np.nan, 67850, np.nan], [10298, np.nan,46138, np.nan], [np.nan, 173500,np.nan, 962784]]\n",
    "\n",
    "check_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Level 3 Filters: IV Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Methodology*\n",
    "\n",
    "The process for construction of the IV filter is as follows:\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li> The IV filter removes volatility outliers to reduce the prevalence of apparent butterfly arbitrage. This involves dropping calls and puts that have the same expiration date and strike price, but have anomalous prices due to extreme implied volatility values. \n",
    "\n",
    "<li> For each *date* and *maturity*, we fit a quadratic curve to the implied volatility of puts and calls (separately) through the observed log implied volatilities\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_range = '1996-01_2012-01'\n",
    "date_range = '2012-02_2019-12'\n",
    "\n",
    "\n",
    "# read in L2 filtered data\n",
    "l2_data = pd.read_parquet(DATA_DIR / f\"intermediate/data_{date_range}_L2filter.parquet\", columns=['secid', 'date', 'exdate', 'cp_flag', 'mnyns', 'impl_volatility', 'tb_m3', 'best_bid', 'best_offer', 'strike_price', 'contract_size', 'sec_price'])\n",
    "l2_data\n",
    "# calc log IV \n",
    "l2_data['log_iv'] = np.log(l2_data['impl_volatility'])\n",
    "l2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(12,8))\n",
    "\n",
    "ax[0,0].hist(l2_data['impl_volatility'], bins=250, color='darkblue')\n",
    "ax[0,0].set_xlabel('IV')\n",
    "ax[0,0].set_ylabel('Frequency')\n",
    "ax[0,0].set_title('Distribution of IV')\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].hist(l2_data['log_iv'], bins=250, color='grey')\n",
    "ax[0,1].set_xlabel('log(IV)')\n",
    "ax[0,1].set_ylabel('Frequency')\n",
    "ax[0,1].set_title('Distribution of log(IV)')\n",
    "ax[0,1].grid()\n",
    "\n",
    "# options with nan implied volatility\n",
    "# calls only\n",
    "nan_iv = l2_data[(l2_data['cp_flag'] == 'C') & (l2_data['impl_volatility'].isna())]\n",
    "ax[1,0].scatter(x=nan_iv['date'], y=nan_iv['mnyns'], color='blue', alpha=0.1, s=10, label='Calls')\n",
    "\n",
    "# puts only\n",
    "nan_iv = l2_data[(l2_data['cp_flag'] == 'P') & (l2_data['impl_volatility'].isna())]\n",
    "ax[1,0].scatter(x=nan_iv['date'], y=nan_iv['mnyns'], color='red', alpha=0.1, s=10, label='Puts')\n",
    "\n",
    "ax[1,0].set_xlabel('Trade Date')\n",
    "ax[1,0].set_ylabel('Moneyness')\n",
    "ax[1,0].set_title('Moneyness of Calls with NaN IV')\n",
    "ax[1,0].grid()\n",
    "ax[1,0].legend()\n",
    "ax[1,0].grid()\n",
    "\n",
    "\n",
    "# percentage of NaN IV\n",
    "nan_percentage = l2_data.groupby(['date', 'cp_flag'])['impl_volatility'].apply(lambda x: (x.isna().sum() / len(x))*100)\n",
    "\n",
    "# calls only\n",
    "nan_percentage_calls = nan_percentage[nan_percentage.index.get_level_values(1)=='C']\n",
    "ax[1,1].scatter(x=nan_percentage_calls.index.get_level_values(0), y=nan_percentage_calls.values, color='blue', alpha = 0.1, s=10, label='Calls')\n",
    "\n",
    "# puts only\n",
    "nan_percentage_puts = nan_percentage[nan_percentage.index.get_level_values(1)=='P']\n",
    "ax[1,1].scatter(x=nan_percentage_puts.index.get_level_values(0), y=nan_percentage_puts.values, color='red', alpha = 0.1, s=10, label='Puts')\n",
    "\n",
    "ax[1,1].set_xlabel('Trade Date')\n",
    "ax[1,1].set_ylabel('Percentage of NaN IV')\n",
    "ax[1,1].set_title('Percentage of NaN IV by Trade Date')\n",
    "ax[1,1].legend()\n",
    "ax[1,1].grid()\n",
    "\n",
    "# Hide ax[1,2]\n",
    "#ax[1,2].axis('off')\n",
    "\n",
    "plt.suptitle('Level 2 Filtered Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noting NaN IV Options in the Level 2 Filtered Data\n",
    "We note that after the Level 2 filters, there are still several NaNs in the IVs of the remaining options. On any given trade date, the number of NaN IV options reach as high as 50% of the options on a given date. It is also interesting to note that the NaN IVs typically are in options that not near-the-money. However, as we see below, the total number of NaN IV options over the entire dataseries is 3.76% for calls, and 5.67% for puts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_iv_calls = l2_data[(l2_data['cp_flag'] == 'C') & (l2_data['impl_volatility'].isna())]\n",
    "nan_iv_puts = l2_data[(l2_data['cp_flag'] == 'P') & (l2_data['impl_volatility'].isna())]\n",
    "nan_iv_summary = pd.DataFrame(index=['Calls', 'Puts'], columns = ['NaN IV Records', 'Total Records', '% NaN IV'])\n",
    "nan_iv_summary.loc['Calls'] = [len(nan_iv_calls), len(l2_data[l2_data['cp_flag'] == 'C']), len(nan_iv_calls)/len(l2_data[l2_data['cp_flag'] == 'C'])*100]\n",
    "nan_iv_summary.loc['Puts'] = [len(nan_iv_puts), len(l2_data[l2_data['cp_flag'] == 'P']), len(nan_iv_puts)/len(l2_data[l2_data['cp_flag'] == 'P'])*100]\n",
    "nan_iv_summary.style.format({'NaN IV Records': '{:,.0f}',\n",
    "                             'Total Records': '{:,.0f}',\n",
    "                             '% NaN IV': '{:.2f}%'}).set_caption('Summary of NaN IV Records in Level 2 Filtered Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing IV Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the Level 3 filters is to remove IV outliers from the dataset. To do this, we fit quadratic curves to each set of calls and puts, grouped by date (`date`) and maturity (`exdate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the quadratic curve fitting function to the data\n",
    "l2_data = l2_data.dropna(subset=['mnyns', 'log_iv']).groupby(['date', 'exdate', 'cp_flag']).filter(lambda group: len(group) >= 3)\n",
    "    \n",
    "l2_data = l2_data.groupby(['date', 'exdate', 'cp_flag']).apply(fit_and_store_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(12,8))\n",
    "\n",
    "ax[0,0].hist(l2_data['impl_volatility'], bins=250, color='darkblue')\n",
    "ax[0,0].set_xlabel('IV')\n",
    "ax[0,0].set_ylabel('Frequency')\n",
    "ax[0,0].set_title('Distribution of IV')\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].hist(l2_data['log_iv'], bins=250, color='grey')\n",
    "ax[0,1].set_xlabel('log(IV)')\n",
    "ax[0,1].set_ylabel('Frequency')\n",
    "ax[0,1].set_title('Distribution of log(IV)')\n",
    "ax[0,1].grid()\n",
    "\n",
    "# Scatter plot of IV vs fitted IV\n",
    "ax[0,2].scatter(x=l2_data['log_iv'], y=l2_data['fitted_iv'], color='darkblue', alpha=0.1)\n",
    "ax[0,2].set_xlabel('log(IV)')\n",
    "ax[0,2].set_ylabel('Fitted log(IV)')\n",
    "ax[0,2].set_title('Fitted log(IV) vs  Observed log(IV)')\n",
    "# Add 45-deg line\n",
    "ax[0,2].plot([min(l2_data['log_iv']), max(l2_data['log_iv'])], [min(l2_data['log_iv']), max(l2_data['log_iv'])], color='red', linestyle='--')\n",
    "ax[0,2].grid()\n",
    "\n",
    "\n",
    "ax[1,0].scatter(x=l2_data.xs('C', level='cp_flag')['mnyns'], y=np.exp(l2_data.xs('C', level='cp_flag')['log_iv']), color='blue', alpha=0.1, label='IV')\n",
    "ax[1,0].set_xlabel('Moneyness')\n",
    "ax[1,0].set_ylabel('IV')\n",
    "ax[1,0].set_title('IV vs Moneyness (Calls)')\n",
    "\n",
    "ax[1,1].scatter(x=l2_data.xs('P', level='cp_flag')['mnyns'], y=np.exp(l2_data.xs('P', level='cp_flag')['log_iv']), color='red', alpha=0.1, label='IV')\n",
    "ax[1,1].set_xlabel('Moneyness')\n",
    "ax[1,1].set_ylabel('IV')\n",
    "ax[1,1].set_title('IV vs Moneyness (Puts)')\n",
    "\n",
    "# Hide ax[1,2]\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "plt.suptitle('Level 2 Filtered Data with Fitted IVs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_data_iv_only = iv_filter_outliers(l2_data, 'percent', 2.0)\n",
    "# convert mnyns_bin to string to save\n",
    "l3_data_iv_only['mnyns_bin'] = l3_data_iv_only['mnyns_bin'].astype(str)\n",
    "l3_data_iv_only.to_parquet(DATA_DIR / f\"intermediate/data_{date_range}_L3filterIVonly.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = check_results.loc[:,'OptionMetrics'].copy(deep=True)\n",
    "final_result.loc[('Level 3 filters', 'IV filter'), 'Deleted'] = len(l2_data)-len(l3_data_iv_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(12,12))\n",
    "\n",
    "ax[0,0].hist(l3_data_iv_only['impl_volatility'], bins=250, color='darkblue')\n",
    "ax[0,0].set_xlabel('IV')\n",
    "ax[0,0].set_ylabel('Frequency')\n",
    "ax[0,0].set_title('Distribution of IV')\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].hist(l3_data_iv_only['log_iv'], bins=250, color='grey')\n",
    "ax[0,1].set_xlabel('log(IV)')\n",
    "ax[0,1].set_ylabel('Frequency')\n",
    "ax[0,1].set_title('Distribution of log(IV)')\n",
    "ax[0,1].grid()\n",
    "\n",
    "# Scatter plot with x=log_iv and y=fitted_iv\n",
    "ax[0,2].scatter(x=l3_data_iv_only['log_iv'], y=l3_data_iv_only['fitted_iv'], color='darkblue', alpha=0.1)\n",
    "ax[0,2].set_xlabel('log(IV)')\n",
    "ax[0,2].set_ylabel('Fitted log(IV)')\n",
    "ax[0,2].set_title('log(IV) vs Fitted log(IV)')\n",
    "# Add a 45-degree line\n",
    "ax[0,2].plot([min(l3_data_iv_only['log_iv']), max(l3_data_iv_only['log_iv'])], [min(l3_data_iv_only['log_iv']), max(l3_data_iv_only['log_iv'])], color='red', linestyle='--')\n",
    "ax[0,2].grid()\n",
    "\n",
    "\n",
    "ax[1,0].scatter(x=l3_data_iv_only.xs('C', level='cp_flag')['mnyns'], y=np.exp(l3_data_iv_only.xs('C', level='cp_flag')['log_iv']), color='blue', alpha=0.1, label='IV')\n",
    "ax[1,0].set_xlabel('Moneyness')\n",
    "ax[1,0].set_ylabel('IV')\n",
    "ax[1,0].set_title('IV vs Moneyness (Calls)')\n",
    "\n",
    "ax[1,1].scatter(x=l3_data_iv_only.xs('P', level='cp_flag')['mnyns'], y=np.exp(l3_data_iv_only.xs('P', level='cp_flag')['log_iv']), color='red', alpha=0.1, label='IV')\n",
    "ax[1,1].set_xlabel('Moneyness')\n",
    "ax[1,1].set_ylabel('IV')\n",
    "ax[1,1].set_title('IV vs Moneyness (Puts)')\n",
    "\n",
    "\n",
    "ax[2,0].scatter(x=l3_data_iv_only.xs('C', level='cp_flag')['mnyns'], y=l3_data_iv_only.xs('C', level='cp_flag')['rel_distance_iv'], color='blue', alpha=0.1, label='Calls')\n",
    "ax[2,0].set_xlabel('Moneyness')\n",
    "ax[2,0].set_ylabel('Relative Distance %')\n",
    "ax[2,0].set_title('Rel. Distance of logIV-fitted IV vs Mnyns (Calls)')\n",
    "ax[2,0].grid()\n",
    "\n",
    "ax[2,1].scatter(x=l3_data_iv_only.xs('P', level='cp_flag')['mnyns'], y=l3_data_iv_only.xs('P', level='cp_flag')['rel_distance_iv'], color='red', alpha=0.1, label='Puts')\n",
    "ax[2,1].set_xlabel('Moneyness')\n",
    "ax[2,1].set_ylabel('Relative Distance %')\n",
    "ax[2,1].set_title('Rel. Distance of logIV-fitted IV vs Mnyns (Puts)')\n",
    "ax[2,1].grid()\n",
    "\n",
    "# hide unused subplots\n",
    "ax[1,2].axis('off')\n",
    "ax[2,2].axis('off')\n",
    "\n",
    "plt.suptitle('Level 3 Filtered Data: IV Filter Only')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_rel_dist = l3_data_iv_only[(l3_data_iv_only['mnyns'] < 1.1) & (l3_data_iv_only['mnyns'] > 0.9)].describe()['rel_distance_iv'].to_frame().rename(columns={'rel_distance_iv': 'Near-The-Money'})\n",
    "fftm_rel_dist = l3_data_iv_only[(l3_data_iv_only['mnyns'] > 1.1) | (l3_data_iv_only['mnyns'] < 0.9)].describe()['rel_distance_iv'].to_frame().rename(columns={'rel_distance_iv': 'Far-From-The-Money Options'})\n",
    "rel_dist_stats = pd.concat([ntm_rel_dist, fftm_rel_dist], axis=1)\n",
    "display(rel_dist_stats.style.format('{:,.2f}').set_caption('Relative Distance Stats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the relative distance estimate using the \"percent\" approach with a 2-standard deviation outlier cutoff threshold is nowhere near what the paper authors indicate (2% near the money, and 3.5% far from the money), as opposed to *-1%* near the money, and *50%* far from the money. Later in this analysis, we will present a sensitivity of the number of options deleted from the dataset with different approaches to distance and different thresholds for the outlier standard deviation cutoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3 filter: Put-Call Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Methodology*\n",
    "\n",
    "The process for construction of the Put-Call Parity filter is as follows:\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li> The puts and calls need to be matched up based on trading date, expiry date, and option type.\n",
    "\n",
    "<li> We then calculate the put-call parity implied interest rate, and filter out outliers based on the standard deviation of the relative distance between the PCP implied interest rate, and the calculated daily median 3-month T-bill rate from the pulled data. \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_data = l3_data_iv_only.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bid-Ask Midpoint Price and Matching Put-Call Pairs\n",
    "- Next we calculate the bid-ask midpoint for each option in the dataset, and match put-call pairs. To do this, we need to ensure that for each call option, there's a corresponding put option with the same *strike price* and *expiration date*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bid-ask midpoint\n",
    "l3_data['mid_price'] = (l3_data['best_bid'] + l3_data['best_offer']) / 2\n",
    "# extract all the call options\n",
    "call_options = l3_data.xs('C', level='cp_flag')\n",
    "call_options\n",
    "# extract all the put options\n",
    "put_options = l3_data.xs('P', level='cp_flag')\n",
    "put_options\n",
    "matching_calls, matching_puts = build_put_call_pairs(call_options.reset_index(drop=True), put_options.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put-Call Parity Implied Interest Rate\n",
    "- We now calculate the put-call parity implied interest rate, which can be achieved using: \n",
    "$$C-P=S-Ke^{rT}$$\n",
    "$$e^{rT}=\\frac{(S-C+P)}{K}$$\n",
    "$$r=\\frac{1}{T}\\cdot log(\\frac{S-C+P}{K})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the puts and calls\n",
    "matched_options = pd.merge(matching_calls, matching_puts, on=['date', 'exdate', 'mnyns'], suffixes=('_C', '_P'))\n",
    "# calculate the PCP implied interest rate \n",
    "matched_options = calc_implied_interest_rate(matched_options)\n",
    "matched_options[matched_options['tb_m3_C'].eq(matched_options['tb_m3_P']) == False][['tb_m3_C', 'tb_m3_P']].isna().sum()\n",
    "\n",
    "# Calculate the daily median implied interest rate from the T-Bill data (same for calls and puts on a given day)\n",
    "daily_median_int_rate = matched_options.groupby('date')['tb_m3_C'].median().reset_index(name='daily_median_rate')\n",
    "matched_options = matched_options.join(daily_median_int_rate.set_index('date'), on='date')\n",
    "matched_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_filtered_options = pcp_filter_outliers(matched_options, 'percent', 2.0)\n",
    "l3_filtered_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.loc[('Level 3 filters', 'Put-call parity filter'), 'Deleted'] = len(l3_data_iv_only)-len(l3_filtered_options)\n",
    "final_result.loc[('Level 3 filters', 'All'), 'Remaining'] = len(l3_filtered_options)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_filter_final_result = pd.merge(final_result, check_results.loc[:, 'OptionMetrics'], left_index=True, right_index=True, suffixes=(' - HamHolDes', ' - OptionMetrics')).style.format('{:,.0f}')\n",
    "l3_filter_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_filtered_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_filtered_options.reset_index().set_index(['date', 'exdate', 'cp_flag'])\n",
    "l3_filtered_options.to_parquet(DATA_DIR / f\"intermediate/data_{date_range}_L3filter.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(12,12))\n",
    "\n",
    "chart_data = l3_filtered_options.reset_index().set_index(['date', 'exdate', 'cp_flag'])\n",
    "\n",
    "ax[0,0].hist(chart_data['impl_volatility'], bins=250, color='darkblue')\n",
    "ax[0,0].set_xlabel('IV')\n",
    "ax[0,0].set_ylabel('Frequency')\n",
    "ax[0,0].set_title('Distribution of IV')\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].hist(chart_data['log_iv'], bins=250, color='grey')\n",
    "ax[0,1].set_xlabel('log(IV)')\n",
    "ax[0,1].set_ylabel('Frequency')\n",
    "ax[0,1].set_title('Distribution of log(IV)')\n",
    "ax[0,1].grid()\n",
    "\n",
    "# Scatter plot with x=log_iv and y=fitted_iv\n",
    "ax[0,2].scatter(x=chart_data['log_iv'], y=chart_data['fitted_iv'], color='darkblue', alpha=0.1)\n",
    "ax[0,2].set_xlabel('log(IV)')\n",
    "ax[0,2].set_ylabel('Fitted log(IV)')\n",
    "ax[0,2].set_title('log(IV) vs Fitted log(IV)')\n",
    "# Add a 45-degree line\n",
    "ax[0,2].plot([min(chart_data['log_iv']), max(chart_data['log_iv'])], [min(chart_data['log_iv']), max(chart_data['log_iv'])], color='red', linestyle='--')\n",
    "ax[0,2].grid()\n",
    "\n",
    "\n",
    "ax[1,0].scatter(x=chart_data.xs('C', level='cp_flag')['mnyns'], y=np.exp(chart_data.xs('C', level='cp_flag')['log_iv']), color='blue', alpha=0.1, label='IV')\n",
    "ax[1,0].set_xlabel('Moneyness')\n",
    "ax[1,0].set_ylabel('IV')\n",
    "ax[1,0].set_title('IV vs Moneyness (Calls)')\n",
    "\n",
    "ax[1,1].scatter(x=chart_data.xs('P', level='cp_flag')['mnyns'], y=np.exp(chart_data.xs('P', level='cp_flag')['log_iv']), color='red', alpha=0.1, label='IV')\n",
    "ax[1,1].set_xlabel('Moneyness')\n",
    "ax[1,1].set_ylabel('IV')\n",
    "ax[1,1].set_title('IV vs Moneyness (Puts)')\n",
    "\n",
    "\n",
    "ax[2,0].scatter(x=chart_data.xs('C', level='cp_flag')['mnyns'], y=chart_data.xs('C', level='cp_flag')['rel_distance_iv'], color='blue', alpha=0.1, label='Calls')\n",
    "ax[2,0].set_xlabel('Moneyness')\n",
    "ax[2,0].set_ylabel('Relative Distance %')\n",
    "ax[2,0].set_title('Rel. Distance of logIV-fitted IV vs Mnyns (Calls)')\n",
    "ax[2,0].grid()\n",
    "\n",
    "ax[2,1].scatter(x=chart_data.xs('P', level='cp_flag')['mnyns'], y=chart_data.xs('P', level='cp_flag')['rel_distance_iv'], color='red', alpha=0.1, label='Puts')\n",
    "ax[2,1].set_xlabel('Moneyness')\n",
    "ax[2,1].set_ylabel('Relative Distance %')\n",
    "ax[2,1].set_title('Rel. Distance of logIV-fitted IV vs Mnyns (Puts)')\n",
    "ax[2,1].grid()\n",
    "\n",
    "# hide unused subplots\n",
    "ax[1,2].axis('off')\n",
    "ax[2,2].axis('off')\n",
    "\n",
    "plt.suptitle('Level 3 Filtered Data: IV+Put-Call Parity Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity of the Level 3 Filters\n",
    "- The options deleted at each stage of the Level 3 filters (as described in the paper) are very sensitive to the distance function utilized and the outlier threshold. Below we analyze the sensitivity of the number of options deleted to variation in the distance function (percent distance, Manhattan (absolute) distance, and Euclidean distance), as well as to a range of 2 to 5 standard deviations to drop outliers. \n",
    "- Since similar filters are used for the **Implied Volatility Filter** as well as the **Put-Call Parity Filter**, errors in filter construction can have significant downstream effects, as we see in the sensitivity table results below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_functions = ['percent', 'manhattan', 'euclidean']\n",
    "outlier_thresholds = [2., 2.5, 3., 3.5, 4., 4.5, 5.] # stdevs\n",
    "\n",
    "# dataframe to store sensitivity results\n",
    "pcp_filter_sens = pd.DataFrame(index=pd.MultiIndex.from_product([distance_functions, outlier_thresholds]),\n",
    "                                      columns=['Deleted', 'Remaining'])\n",
    "pcp_filter_sens.index.names = ['Rel. Dist. Method', 'Outlier Stdev Thresh.']\n",
    "\n",
    "iv_filter_sens = pd.DataFrame(index=pd.MultiIndex.from_product([distance_functions, outlier_thresholds]),\n",
    "                                      columns=['Deleted', 'Remaining'])\n",
    "iv_filter_sens.index.names = ['Rel. Dist. Method', 'Outlier Stdev Thresh.']\n",
    "\n",
    "# run sensitivity analysis\n",
    "for dist in distance_functions:\n",
    "    for thresh in outlier_thresholds:\n",
    "        print('Running sensitivity analysis for distance function:', dist, 'and outlier threshold:', thresh)\n",
    "        l3_data_iv_only = iv_filter_outliers(l2_data, dist, thresh)\n",
    "        iv_filter_sens.loc[(dist, thresh), :] = {'Deleted' : len(l2_data)-len(l3_data_iv_only),\n",
    "                                                 'Remaining': len(l3_data_iv_only)}\n",
    "        \n",
    "        l3_filtered_options = pcp_filter_outliers(matched_options, dist, thresh)\n",
    "        pcp_filter_sens.loc[(dist, thresh), :] = {'Deleted' : len(l3_data_iv_only) - len(l3_filtered_options),\n",
    "                                                                                 'Remaining': len(l3_filtered_options)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_sens_table = pd.merge(iv_filter_sens, pcp_filter_sens, left_index=True, right_index=True, suffixes=(' - Post-IV', ' - Post-PCP'))\n",
    "l3_sens_table.style.format('{:,.0f}')\n",
    "l3_sens_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_in_iv_deletions = (l3_sens_table['Deleted - Post-IV'] / check_results.loc[('Level 3 filters', 'IV filter'), 'OptionMetrics']['Deleted'] - 1).to_frame().rename(columns={'Deleted - Post-IV': '% Error in IV Deletions'})\n",
    "error_in_pcp_deletions = (l3_sens_table['Deleted - Post-PCP'] / check_results.loc[('Level 3 filters', 'Put-call parity filter'), 'OptionMetrics']['Deleted'] - 1).to_frame().rename(columns={'Deleted - Post-PCP': '% Error in PCP Deletions'})\n",
    "error_in_final_res = (l3_sens_table['Remaining - Post-PCP'] / check_results.loc[('Level 3 filters', 'All'), 'OptionMetrics']['Remaining'] - 1).to_frame().rename(columns={'Remaining - Post-PCP': '% Error Final Option Count'})\n",
    "\n",
    "error_in_deletions = pd.concat((error_in_iv_deletions, error_in_pcp_deletions, error_in_final_res), axis=1)\n",
    "display(error_in_deletions.style.format('{:.1%}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on Final Result\n",
    "- Our final option set seems to distributed in a fairly well-behaved volatility curve, when plotted against moneyness, with a prominent \"volatility smirk\" for options with moneyness < 1.0. \n",
    "- We note that the final option counts from our implementation are mostly within 5% - 10% of Table B1 in the paper, which is surprisingly close, given the number of judgment calls that needed to be made with regard to the filter methodology. \n",
    "- This suggests that further efforts to reproduce the paper's results might be challenging, since it is quite likely that the specific composition of options in our final dataset might not correspond exactly with those in the paper, despite our best efforts at reproducing the filters as described. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implied Volatilities based on Put-Call Parity Implied Interest Rate\n",
    "Next we compute implied volatilities based on the put-call parity implied interest rate, and compare them to the T-bill implied volatilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
